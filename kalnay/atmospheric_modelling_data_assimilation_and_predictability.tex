\chapter{Atmospheric Modelling, Data Assimilation and Predictability}
\label{ch:kalnay}

The following constitutes the notes that I have made whilst going through Atmospheric Modeling, Data Assimilation and Predictability\citep{kalnay_atmospheric_2003}.

\section{Historical overview of numerical weather prediction}
\label{sec:kalnay:historical_overview}

\subsection{Introduction}
\label{sub:historical_overview:introduction}

The improvement in skill of numerical weather prediction over the last 40 years is due to the following factors:
\begin{itemize}
\item The increased power of supercomputers - this allows for finer numerical resolution, and means that fewer approximations are required in the operational atmospheric models;
\item The improved representation of small-scale physical processes - these include clouds, precipitation, turbulent transfer processes;
\item The use of more accurate methods of data assimilation - these lead to improved initial conditions for the models;
\item The increased availability of data - for example, satellite and aircraft data over the oceans and the Southern Hemisphere. 
\end{itemize}

\subsection{Early developments}
\label{sub:historical_overview:early_developments}

\subsection{Primitive equations, global and regional models, and nonhydrostatic models}
\label{sub:historical_overview:primitive_equations}

Whilst quasi-geostrophic equations may be useful for understanding the large-scale extratropical dynamics of the atmosphere, they are not accurate enough to allow for continued progress in Numerical Weather Prediction (NWP).
They are therefore replaced by the primitive equations.
The primitive equations are conservation laws applied to individual parcels of air:
\begin{itemize}
\item Conservation of three-dimensional momentum - the equations of motion;
\item Conservation of energy - the first law of thermodynamics;
\item Conservation of dry air mass - the continuity equation;
\item Conservation of moisture in all of its phases;
\item Ideal gas law.
\end{itemize}
Unlike the quasi-geostrophic equation previously used, these equations capture fast gravity and sound waves; however this requires that finer time discretisation is used (or some alternative method).

Continuous equations of motion are solved by discretisation in space and time.
The accuracy of a model is strongly influenced by the spatial resolution: typically, the higher the resolution, the more accurate the model.
This increase in resolution incurs greater computational cost though.
Whilst modern numerical methods have attempted to alleviate this cost (using semi-implicit and semi-Lagrangian time schemes), the continued increase in spatial resolution is effectively an endless ``race to the bottom'', with constant demand for higher resolution to improve NWP forecasts.

\subsection{Data assimilation determination of the initial conditions for the computer forecasts}
\label{sub:historical_overview:assimilation}

\begin{itemize}
    \item NWP is an intial-value problem --- given an estimated starting state, the model forecasts the system's evolution --- as therefore, the determination of the initial conditions is very imporant.
    \item This hass been achieved through:
    \begin{itemize}
        \item successive correction methods (SCM),
        \item optimal interpolation (OI),
        \item variational methods in three and four dimensions (3D-Var, 4D-Var),
        \item Kalman filtering (KF).
    \end{itemize}
    \item Additional information is required to prepare the intial conditions for forecasts, which we call `background', `first guess' or `prior information'.
    \item Terminology for this section:
    \begin{itemize}
        \item $x^b$: a three-dimensional array, `background field',
        \item $y^o$: `observed variables',
        \item $H$: `observation operator' --- performs the necessary interpolation and transformation from model variables to observation space,
        \item $H \left( x^b \right)$: `first guess observations',
        \item $y^o - H \left( x^b \right)$: `observation increments' or `innovations' --- the difference between the observations and the model first guess,
        \item $W$: weights,
        \item $x^a$: `the analysis' --- adding the innovations to the model forecast with weights $W$ based on the estimated statistacl error covariances of forecast and observations:
        \begin{equation}
            \mathbf{x^a} = \mathbf{x^b} + \mathbf{W} 
            \left[ \mathbf{y^o} - \mathbf{H} \left( \mathbf{x^b} \right) \right]
        \end{equation}
        This acts as the basis for various analysis schemes
        \item SCM: weights are empirically determined, as a function of distance between observation and grid point.
        \item OI: matrix of weights is determined from the minimisation of analysis errors at each grid point.
        \item 3D-Var: Defining cost function proportional to the square of the distance between the analysis and the background and observations, and aim to minimise it.
        \item Differences between OI and 3D-Var: in OI weights W are obtained using simplifications; in 3D-Var minimisation is performed directly.
        \item 4D-Var: lookat minimisation problem from 3D-Var but also considering distance to observations over time window.
    \end{itemize}
\end{itemize}

\subsection{Weather predictability, ensemble forecasting, and seasonal to interannual prediction}
\label{sub:historical_overview:predictability}

\begin{itemize}
    \item Given that the skill of forecasting decreases with time, it is suggested that stochastic forecasting should be favoured over deterministic forecasting, providing an estimate of the prediction.
    \item This is achieved through ensemble forecasting, where multiple models are run forward to forecast, with small perturbations being introduced.
    \item The result is an ensemble average forecast that is typically more accurate than individual forecasts beyond the first few days; furthermore, it provides forecasters with a measure of reliability.
    \item This system can also be used to develop adaptive and targeted observation networks.
\end{itemize}

% \section{The continuous equations}
% \label{sec:kalnay:continuous_equations}

% \section{Numerical discretization of the equations of motion}
% \label{sec:kalnay:numerical_discretization}

% \section{Introduction to the parameterization of subgrid-scale physical processes}
% \label{sec:kalnay:parameterization}

\section{Data assimilation}
\label{sec:kalnay:data_assimilation}

\subsection{Introduction}
\label{sub:data_assimilation:intro}

\begin{itemize}
    \item NWP is an initial value problem.
    \item Data assimilation is the name given to the statistical approach to combining observations and forecasts.
\end{itemize}

\subsection{Empirical analysis schemes}
\label{sub:data_assimilation:analysis:empirical}

\subsubsection{Successive corrections method (SCM)}
\label{subs:empirical:scm}

\begin{itemize}
    \item First estimate of gridded field is given by the background (i.e. first guess) field:
    \begin{equation}
        f_i^0 = f_i^b
    \end{equation}
    where $f_i^b$ is the background evaluated at the $i$, and $f_i^0$ is the corresponding zeroth iteration estimate of the gridded field.
    \item Subsequent iterations are obtained through ``successive corrections'':
    \begin{equation}
        f_i^{n+1} = f_i^n + 
        \frac{\sum_{k=1}^{K_i^n} w_{ik}^n \left( f_k^O - f_k^n \right)}
        {\sum_{k=1}^{K_i^n} w_{ik}^n + \varepsilon^2}
    \end{equation}
    where:
    \begin{itemize}
        \item $f_i^n$: $n$th iteration estimate at grid point $i$,
        \item $f_k^O$: $k$th observation surrounding grid point $i$,
        \item $f_k^n$: $n$th field estimate evaluated at observation point $k$ (through interpolation),
        \item $\varepsilon^2$: estimate of the ratio of observation error variance to background error variance.
    \end{itemize}
    \item The weights, $w_{ik}^n$ can be defined:
    \begin{equation}
        w_{ik}^n =
        \begin{cases}
            \frac{R_n^2 - r_{ik}^2}{R_n^2 + r_{ik}^2}       & \quad \text{for } r_{ik}^2 \leq R_n^2
            \\
            0  & \quad \text{for } r_{ik}^2 > R_n^2
        \end{cases}
    \end{equation}
    where $r_{ik}^2$ is the square of the distance between an observation point $\mathbf{r}_k$ and a grid point $\mathbf{r}_i$. 
    \item The radius of influence, $R_n$ varies from iteration to iteration.
    \item $K_i^n$ is the number of observations within $R_n$ of grid point $i$.
    \item $\varepsilon^2 > 0$ implies that the observations have errors, giving some weight to the background field.
\end{itemize}

\subsubsection{Nudging}
\label{subs:empirical:nudging}

\begin{itemize}
    \item Another empirical data assimilation method is Newtonian relaxation, also known as nudging.
    \item This involves adding an extra term to the prognostic equations that nudges the solution towards the observations.
    \item A relaxation timescale, $\tau$, is chosen based on empirical considerations.
    \begin{itemize}
        \item If $\tau$ is very small, the solution converges towards the observations too quickly --- the dynamics do not have time to adjust.
        \item If $\tau$ is very large, the errors in the model grow too much before the nudging takes effect.
    \end{itemize}
    \item Consider using a $\tau$ s.t. the last term is of a similar order of magnitude to the less dominant terms.
    \item This method is not typically used for large-scale assimilation (why not?).
    \item Might be used to assimilate small-scale observations when no statistics are available to perform statistical interpolation.
\end{itemize}

\subsection{Introduction to least squares methods}
\label{sub:data_assimilation:squares}

\begin{itemize}
    \item Methods outlined thus far have been deterministic data assimilation methods.
    \item The following methods are based on statistical estimation.
\end{itemize}

\subsubsection{Least squares method}
\label{subs:squares:lsm}

\begin{itemize}
    \item Methods outlined here are given by examples of both sequential and variational approaches.
    \item A similar approach can be undertaken for multivariate optimal interpolation, kalman filtering, and 3D-Var and 4D-Var.
    \item The best estimate of a state is obtained b combining prior information with observations.
    \begin{itemize}
        \item To optimise the combination process, we need statistical information about the errors in each of these.
    \end{itemize}
    \item \textbf{Example: Determining the true temperature, $T_t$, given two independent observations, $T_1$ and $T_2$}
    \begin{itemize}
        \item Each observation, $T_i$, has an associated unknown error, $\varepsilon_i$:
        \begin{subequations}
            \begin{align}
                 T_1 &= T_t + \varepsilon_1, \\
                 T_2 &= T_t + \varepsilon_2
            \end{align}
        \end{subequations}
        \item Assume that the instrument used to measure $T_1$ and $T_2$ are unbiased, i.e.:
        \begin{equation*}
            \mathbb{E}(T_1 - T_t) = E(T_2 - T_t) = 0 
        \end{equation*}
        or
        \begin{equation}
            \mathbb{E}(\varepsilon_1) = \mathbb{E}(\varepsilon_2) = 0
        \end{equation}
        \item Assume that the variances of the observational errors are known:
        \begin{subequations}
            \begin{align}
                \mathbb{E}(\varepsilon_1^2) = \sigma_1^2, \\
                \mathbb{E}(\varepsilon_2^2) = \sigma_2^2
            \end{align}
        \end{subequations}
        \item Assume that the errors of the two measurements are uncorrelated:
        \begin{equation}
            \mathbb{E}(\varepsilon_1 \varepsilon_2) = 0
        \end{equation}
        \item Attempt to estimate $T_t$ using a linear combination of the two observations:
        \begin{equation}
            T_a = a_1 T_1 + a_2 T_2
        \end{equation}
        \item The analysis, $T_a$ should be unbiased, i.e.:
        \begin{equation}
            \mathbb{E}(T_a) = \mathbb{E}(T_t),
        \end{equation}
        and therefore
        \begin{equation}
            a_1 + a_2 = 1. \label{eq:coefficients}
        \end{equation}
        (see Appendix \ref{sec:kalnay_working:least_squares} for details).
        \item In order to make $T_a$ the best estimate of $T_t$, we aim to optimise the coefficients to minimise the mean squared error (MSE) of $T_a$:
        \begin{align}
            \sigma_a^2 &= \mathbb{E} \left( \left( T_a - T_t \right)^2 \right) \nonumber \\
                        &= \mathbb{E} \left( \left( a_1(T_1 - T_t) + a_2(T_2 - T_t) \right)^2 \right) \label{eq:mse}
        \end{align}
        in line with constraints \ref{eq:coefficients}.
        \item Substituting $a_2 = 1 - a_1$ into equation \ref{eq:mse}, the minimisation of $\sigma_a^2$ yields
        \begin{subequations}
            \begin{align}
            a_1 &= \frac{\frac{1}{\sigma_1^2}}{\frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2}}, \\
            a_2 &= \frac{\frac{1}{\sigma_2^2}}{\frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2}},
            \end{align}
        \end{subequations}
        which can be rewritten as
        \begin{subequations}
            \begin{align}
            a_1 &= \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}, \\
            a_2 &= \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2}.
            \end{align}
        \end{subequations}
        as seen in Appendix \ref{sec:kalnay_working:mse}.
        \item  The author takes precision and accuracy to be synonymous, defining them to be the inverse of the observational error, i.e. $\frac{1}{\sigma^2}$. I disagree with this lumping of the two terms. Precision is a measure of the observational error, whilst accuracy is a measure of how close observations are to the true value. The definition here refers to precision, with the level of observational bias indicating the accuracy.
        \item Based on the above definition of precision, we can use equation \ref{eq:mse} to define an expression for the analysis precision:
        \begin{equation}
            \frac{1}{\sigma_a^2} = \frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2} \label{eq:}
        \end{equation}
        (see Appendix \ref{sec:kalnay:analysis} for working).
    \end{itemize}
\end{itemize}

\subsubsection{Variational (cost function) approach}
\label{subs:squares:variational}

\begin{itemize}
    \item We can also obtain the same best estimate of $T_t$ by minimising the cost function, $J$, which is defined as
    \begin{equation}
            J(T) = \frac{1}{2} \left[ \frac{\left( T - T_1 \right)^2}{\sigma_1^2} + \frac{\left( T - T_2 \right)^2}{\sigma_2^2} \right] \label{eq:cost_function}
    \end{equation}
    which is the sum of the squares of the distances of $T$ from each of the observations, weighted by their precision.
    \item The minimum of the cost function is obtained for $T = T_a$ (see Appendix \ref{sec:kalnay_working:cost}).
\end{itemize}
\begin{itemize}
    \item A alternative approach to arrive at this conclusion would be via a \emph{maximum likelihood} approach, whereby we seek to find the most likely value of the true temperature, $T_t$, given the following information:
    \begin{itemize}
        \item Two independent temperature observations, $T_1$ and $T_2$,
        \item Both observations have normally distributed errors, with respective standard deviations $\sigma_1$ and $\sigma_2$,
    \end{itemize}
    \item In this approach, the analysis is defined as the most likely value of $T$ given the observations and their statistical errors.
    \item We define the probability distributions of each of $T_1$ and $T_2$ given the true value $T$ and their respective standard deviations are given by Gaussian distributions:
    \begin{subequations}
        \begin{align}
            p_{\sigma_1} \left( T_1 | T \right) &= \frac{1}{\sqrt{2 \pi} \sigma_1} e^{-\frac{\left( T_1 - T \right)^2}{2 \sigma_1^2}}, \\
            p_{\sigma_2} \left( T_2 | T \right) &= \frac{1}{\sqrt{2 \pi} \sigma_2} e^{-\frac{\left( T_2 - T \right)^2}{2 \sigma_2^2}}.
        \end{align}
    \end{subequations}
    \item The likelihood of the true value, $T$, given an observation, $T_i$, with standard deviation $\sigma_i$, is given by
    \begin{equation}
        \mathcal{L}_{\sigma_i} \left( T || T_i \right) = 
            p_{\sigma_i} \left( T_i | T \right) =
            \frac{1}{\sqrt{2 \pi}\sigma_i} e^{-\frac{\left( T_i - T \right)^2}{2 \sigma_i^2}}
    \end{equation}
\end{itemize}

\subsubsection{Simplest sequential assimilation and Kalman filtering for a scalar}
\label{subs:squares:sequential}

\subsection{Multivariate statistical data assimilation methods}
\label{sub:data_assimilation:multivar}

\subsection{3D-Var, the physical space analysis scheme (PSAS), and their relation to OI}
\label{sub:data_assimilation:3d-var}

\subsection{Advanced data assimilation methods with evolving forecast error covariance}
\label{sub:data_assimilation:advanced}

\subsection{Dynamical and physical balance in the initial conditions}
\label{sub:data_assimilation:balance}

\subsection{Quality control of observations}
\label{sub:data_assimilation:quality}


% \section{Atmospheric predictability and ensemble forecasting}
% \label{sec:kalnay:predictability}

