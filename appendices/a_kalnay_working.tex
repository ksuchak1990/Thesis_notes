\chapter{Checking Kalnay working}
\label{app:kalnay_working}

\section{Working for least squares coefficients}
\label{sec:kalnay_working:least_squares}

Showing that the coefficients $a_1$ and $a_2$ fulfil
\begin{equation}
a_1 + a_2 = 1
\end{equation}

\subsection{Assumptions}
\label{sub:least_squares:assumptions}

\begin{itemize}
    \item $T_1$ and $T_2$ are unbiased:
    \begin{equation}
        \mathbb{E}(T_1 - T_t) = \mathbb{E}(T_2 - T_t) = 0
    \end{equation}{}
    \item $T_a$ is unbiased:
    \begin{equation}
        \mathbb{E}(T_a) = \mathbb{E}(T_t)
    \end{equation}
\end{itemize}

\subsection{Working}
\label{sub:least_squares:working}

We estimate $T_t$ with a linear combination of the two observations:
\begin{equation*}
    T_a = a_1 T_1 + a_2 T_2,
\end{equation*}
and thus
\begin{equation*}
    \mathbb{E}(T_a) = \mathbb{E}(a_1 T_1 + a_2 T_2).
\end{equation*}
Given that
\begin{equation*}
    \mathbb{E}(X + Y) = \mathbb{E}(X) + \mathbb{E}(Y),
\end{equation*}
we have
\begin{equation*}
    \mathbb{E}(T_a) = \mathbb{E}(a_1 T_1) + \mathbb{E}(a_2 T_2).
\end{equation*}
Given that
\begin{equation*}
    \mathbb{E}(aX) = a \mathbb{E}(X),
\end{equation*}
we have
\begin{equation*}
    \mathbb{E}(T_a) = a_1 \mathbb{E}(T_1) + a_2 \mathbb{E}(T_2).
\end{equation*}
Given that $T_1$ and $T_2$ are unbiased, we have
\begin{equation*}
    \mathbb{E}(T_a) = a_1 \mathbb{E}(T_t) + a_2 \mathbb{E}(T_t).
\end{equation*}
Given that $T_a$ is unbiased, we have
\begin{equation*}
    \mathbb{E}(T_t) = a_1 \mathbb{E}(T_t) + a_2 \mathbb{E}(T_t),
\end{equation*}
\begin{equation}
    \therefore a_1 + a_2 = 1 \label{eq:coeff_constraints}
\end{equation}

\section{Working for mean square errors minimisation}
\label{sec:kalnay_working:mse}

Showing that minimisation of 
\begin{equation}
\sigma_a^2 = \mathbb{E} \left( \left( a_1(T_1 - T_t) + a_2(T_2 - T_t) \right)^2 \right)
\end{equation}
yields
\begin{subequations}
    \begin{align}
    a_1 &= \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}, \\
    a_2 &= \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2}.
    \end{align}
\end{subequations}        

\subsection{Assumptions}
\label{sub:mse:assumptions}

\begin{itemize}
    \item The mean squared error (MSE) of $T_a$ is given by
    \begin{equation}
        \sigma_a^2 = \mathbb{E} \left( \left( a_1 \left( T_1 - T_t \right) + a_2 \left( T_2 - T_t \right) \right)^2 \right) \label{eq:ta_mse}
    \end{equation}
    \item Two independent temperature observations are given by
    \begin{subequations}
        \begin{align}
        T_1 &= T_t + \varepsilon_1, \\
        T_2 &= T_t + \varepsilon_2.
        \end{align}
        \label{:eq:t_independence}
    \end{subequations}
    \item Variances of the observation errors are given by
    \begin{subequations}
        \begin{align}
            \mathbb{E}(\varepsilon_1^2) &= \sigma_1^2, \\
            \mathbb{E}(\varepsilon_2^2) &= \sigma_2^2.
        \end{align}
        \label{eq:t_var}
    \end{subequations}
    \item The errors of the two measurements are uncorrelated:
    \begin{equation}
        \mathbb{E}(\varepsilon_1 \varepsilon_2) = 0 \label{eq:t_uncorrelated}
    \end{equation}
\end{itemize}

\subsection{Working}
\label{sub:mse:working}

Starting with equation \ref{eq:ta_mse}, we substitute in equation \ref{eq:coeff_constraints}:
\begin{equation*}
        \sigma_a^2 = \mathbb{E} \left( \left( a_1 \left( T_1 - T_t \right) + (1 - a_1) \left( T_2 - T_t \right) \right)^2 \right).
\end{equation*}
Substituting in equation \ref{:eq:t_independence}
\begin{align*}
        \sigma_a^2 &= \mathbb{E} \left( \left( a_1 \left( T_t + \varepsilon_1 - T_t \right) + (1 - a_1) \left( T_t +\varepsilon_2 - T_t \right) \right)^2 \right) \\
                    &= \mathbb{E} \left( \left( a_1 \varepsilon_1 +\varepsilon_2 - a_1 \varepsilon_2 \right)^2 \right)
\end{align*}
Multiplying out the square,
\begin{equation*}
    \sigma_a^2 = \mathbb{E} \left( a_1^2 \left( \varepsilon_1^2 + \varepsilon_2^2 - 2 \varepsilon_1 \varepsilon_2 \right) + a_1 \left( 2 \varepsilon_1 \varepsilon_2 - 2 \varepsilon_2^2 \right) + \varepsilon_2^2 \right)
\end{equation*}
Distributing the expectation over individual terms
\begin{equation*}
    \sigma_a^2 = a_1^2 \mathbb{E}(\varepsilon_1^2) + a_1^2 \mathbb{E}(\varepsilon_2^2) - 2 a_1^2 \mathbb{E}(\varepsilon_1 \varepsilon_2) + 2 a_1 \mathbb{E}(\varepsilon_1 \varepsilon_2) - 2 a_1 \mathbb{E}(\varepsilon_2^2) + \mathbb{E}(\varepsilon_2^2)
\end{equation*}
Making use of equation \ref{eq:t_uncorrelated},
\begin{equation*}
   \sigma_a^2 = a_1^2 \mathbb{E}(\varepsilon_1^2) + a_1^2 \mathbb{E}(\varepsilon_2^2) - 2 a_1 \mathbb{E}(\varepsilon_2^2) + \mathbb{E}(\varepsilon_2^2)
\end{equation*}
and substituting in equation \ref{eq:t_var},
\begin{align*}
   \sigma_a^2 &= a_1^2 \sigma_1^2 + a_1^2 \sigma_2^2 - 2 a_1 \sigma_2^2 + \sigma_2^2 \\
                &= \left( \sigma_1^2 + \sigma_2^2 \right) a_1^2 - 2 \sigma_2^2 a_1 + \sigma_2^2
\end{align*}
Differentiating with respect to $a_1$:
\begin{equation}
    \frac{\partial \sigma_a^2}{\partial a_1} = 2 \left( \sigma_1^2 + \sigma_2^2 \right) a_1 - 2 \sigma_2^2 \label{eq:derivative1}
\end{equation}
The stationary point is found by setting $\frac{d \sigma_a^2}{da_1} = 0$, and solving for $a_1$:
\begin{equation*}
    2 \left( \sigma_1^2 + \sigma_2^2 \right) a_1 - 2 \sigma_2^2 = 0
\end{equation*}
\begin{equation}
\therefore a_1 = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2} \label{eq:result_a1}
\end{equation}
We confirm that this critical point is, indeed a minimum by taking the second derivative, i.e. differentiating equation \ref{eq:derivative1} with respect to $a_1$:
\begin{equation}
    \frac{\partial^2 \sigma_a^2}{\partial a_1^2} = 2 \left( \sigma_1^2 + \sigma_2^2 \right) \label{eq:derivative2}
\end{equation}
From equation \ref{eq:derivative2}, $\forall a_1$, $\frac{\partial ^2 \sigma_a^2}{\partial a_1^2} > 0$ and therefore $a_1 = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}$ is a minimum.
We use equation \ref{eq:coeff_constraints} to deduce that
\begin{equation}
    a_2 = \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} \label{eq:result_a2}
\end{equation}

\section{Working for analysis accuracy}
\label{sec:kalnay:analysis}
Showing that, based on equation \ref{eq:ta_mse}, the analysis variance fulfils
\begin{equation}
    \frac{1}{\sigma_a^2} = \frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2}
\end{equation}

\subsection{Assumptions}
\label{sub:analysis:assumptions}

\begin{itemize}
    \item Coefficients can be given by
    \begin{subequations}
        \begin{align}
            a_1 &= \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}, \\
            a_2 &= \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2}.
        \end{align}
        \label{eq:coefficients_results}
    \end{subequations}
\end{itemize}

\subsection{Working}
\label{sub:analysis:working}

Starting with equation \ref{eq:ta_mse}, we substitute in equation \ref{:eq:t_independence} which gives
\begin{equation*}
    \sigma_a^2 = \mathbb{E} \left( \left( a_1 \varepsilon_1 + a_2 \varepsilon_2 \right)^2 \right).
\end{equation*}
We then expand the squared term, distribute the expectation and eliminate $0$-terms:
\begin{align*}
    \sigma_a^2 &= \mathbb{E} \left( a_1^2 \varepsilon_1^2 \right) + \mathbb{E} \left( a_2^2 \varepsilon_2^2 \right) \\
                &= a_1^2 \sigma_1^2 + a_2^2 \sigma_2^2
\end{align*}
Substituting in equation \ref{eq:coefficients_results} gives
\begin{align*}
    \sigma_a^2 &= \left( \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2} \right)^2 \sigma_1^2 + \left( \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} \right)^2 \sigma_2^2 \\
                &= \frac{\sigma_1^2 \sigma_2^4 + \sigma_1^4 \sigma_2^2}{\left( \sigma_1^2 + \sigma_2^2 \right)^2} \\
                &= \frac{\sigma_1^2 \sigma_2^2 \left(\sigma_1^2 + \sigma_2^2 \right)}{\left(\sigma_1^2 + \sigma_2^2 \right)^2} \\
                &= \frac{\sigma_1^2 \sigma_2^2}{\sigma_1^2 + \sigma_2^2}
\end{align*}
which, upon inversion, gives
\begin{align}
    \frac{1}{\sigma_a^2} &= \frac{\sigma_1^2 + \sigma_2^2}{\sigma_1^2 \sigma_2^2} \nonumber \\
                            &= \frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2}
\end{align}

\section{Working for cost function minimisation}
\label{sec:kalnay_working:cost}

Showing that the cost function
\begin{equation*}
    J(T) = \frac{}{} \left[ \frac{\left( T - T_1 \right)^2}{\sigma_1^2} + \frac{\left( T - T_1 \right)^2}{\sigma_1^2} \right]
\end{equation*}
is minimised for $T = T_a$ when using the coefficients found in equation \ref{eq:coefficients_results}.

\subsection{Assumptions}
\label{sub:cost:assumptions}

\begin{itemize}
    \item Not too much more to go in here.
\end{itemize}

\subsection{Working}
\label{sub:cost:working}

\begin{itemize}
    \item We start with the cost function
    \begin{equation}
        J(T) = \frac{1}{2} \left[ \frac{\left( T - T_1 \right)^2}{\sigma_1^2} + \frac{\left( T - T_1 \right)^2}{\sigma_1^2} \right]
    \end{equation}
    and expand the squared terms:
    \begin{align*}
        J(T) &= \frac{1}{2} \left[ \frac{T^2 + T_1^2 - 2 T_1 T}{\sigma_1^2} + \frac{T^2 + T_2^2 - 2 T T_2}{\sigma_2^2} \right] \\
                &= \frac{1}{2} \left[ 
                    \left( \frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2} \right) T^2 
                    - 2 \left( \frac{T_1}{\sigma_1^2} + \frac{T_2}{\sigma_2^2} \right) T
                    + \left( \frac{T_1^2}{\sigma_1^2} + \frac{T_2^2}{\sigma_2^2} \right) 
                    \right]
    \end{align*}
    \item Differentiating with respect to $T$ gives:
    \begin{align}
        \frac{\partial J}{\partial T} &= \frac{1}{2} 
        \left[
            2 \left( \frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2} \right) T
            - 2 \left( \frac{T_1}{\sigma_1^2} + \frac{T_2}{\sigma_2^2} \right)
        \right] \nonumber \\
        &= \left( \frac{1}{\sigma_1^2} + \frac{1}{\sigma_2^2} \right) T
            - \left( \frac{T_1}{\sigma_1^2} + \frac{T_2}{\sigma_2^2} \right) \nonumber \\
        &= \left(\frac{\sigma_1^2 + \sigma_2^2}{\sigma_1^2 \sigma_2^2} \right) T
            - \left( \frac{T_1 \sigma_2^2 + T_2 \sigma_1^2}{\sigma_1^2 \sigma_2^2} \right) \nonumber \\
        &= \frac{1}{\sigma_1^2 \sigma_2^2} \left[
            \left( \sigma_1^2 + \sigma_2^2 \right) T 
            - \left( T_1 \sigma_2^2 + T_2 \sigma_1^2 \right)
        \right]
    \end{align}
    \item We find the stationary point by finding the value of $T$ for $\partial_T J = 0$:
    \begin{align}
        0 &= \frac{1}{\sigma_1^2 \sigma_2^2} \left[
            \left( \sigma_1^2 + \sigma_2^2 \right) T 
            - \left( T_1 \sigma_2^2 + T_2 \sigma_1^2 \right)
        \right] \nonumber \\
            &= \left( \sigma_1^2 + \sigma_2^2 \right) T 
                - \left( T_1 \sigma_2^2 + T_2 \sigma_1^2 \right) \nonumber \\
        \therefore T &= \frac{T_1 \sigma_2^2 +T_2 \sigma_1^2}{\sigma_1^2 + \sigma_2^2} \nonumber \\
            &= \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2} T_1
                + \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} T_2 \nonumber \\
            &= a_1 T_1 + a_2 T_2 \nonumber \\
        \therefore T &= T_a  
    \end{align}
    \item To confirm that the stationary point at $T = T_a$ is a minimum, we take $\partial_{TT} J$, i.e. the second derivative of $J$ with respect to $T$:
    \begin{align*}
        \frac{\partial^2 J}{\partial T^2} &= \frac{\sigma_1^2 + \sigma_2^2}{\sigma_1^2 \sigma_2^2} \\
            &> 0 \, \forall \, T,
    \end{align*}
    and therefore the stationary point at $T = T_a$ is a minimum.
\end{itemize}

\section{Bayes' Theorem}
\label{sec:kalnay_working:cost}

Bayes' theorem is stated as
\begin{equation}
    P \left( A | B \right)
    =
    \frac{P \left( B | A \right) P \left( A \right)}{P \left( B \right)}
\end{equation}
where we use the following definitions:
\begin{itemize}
    \item $P \left( A | B \right)$ --- the probability of $A$ given $B$, known as the posterior,
    \item $P \left( B | A \right)$ --- the probability of $B$ given $A$, known as the likelihood,
    \item $P \left( A \right)$ --- the probability of $A$, known as the prior,
    \item $P \left( B \right)$ --- the probability of $B$, known as the marginal likelihood.
\end{itemize}